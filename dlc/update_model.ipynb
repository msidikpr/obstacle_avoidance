{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set path to dlc project config file \n",
    "config_path = r\"D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vid_path = [r\"D:\\obstacle_avoidance\\recordings\\071023\\G8CKRT\\oadark\\071023_G8CKRT_whiteblack_Rig2_oadark_top1.avi\",r\"D:\\obstacle_avoidance\\recordings\\071023\\G8CKRN\\oadark\\071023_G8CKRN_whiteblack_Rig2_oadark_top1.avi\",\n",
    "                r\"D:\\obstacle_avoidance\\recordings\\071023\\G8CKLN\\oadark\\071023_G8CKLN_whiteblack_Rig2_oadark_top1.avi\",r\"D:\\obstacle_avoidance\\recordings\\071023\\G8CKLT\\oadark\\071023_G8CKLT_whiteblack_Rig2_oadark_top1.avi\"\n",
    "                \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-2160000 for model D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\dlc-models\\iteration-3\\project_nameApr28-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071023\\G8CKRT\\oadark\\071023_G8CKRT_whiteblack_Rig2_oadark_top1.avi\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071023\\G8CKRN\\oadark\\071023_G8CKRN_whiteblack_Rig2_oadark_top1.avi\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071023\\G8CKLN\\oadark\\071023_G8CKLN_whiteblack_Rig2_oadark_top1.avi\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071023\\G8CKLT\\oadark\\071023_G8CKLT_whiteblack_Rig2_oadark_top1.avi\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DLC_resnet50_project_nameApr28shuffle1_2160000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## analyze new videos\n",
    "deeplabcut.analyze_videos(config_path, new_vid_path, shuffle=1, save_as_csv=False, videotype='.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:338: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  4341  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  691.9333333333333 , recorded @  60.0 fps!\n",
      "Overall # of frames:  41516 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 691.93  seconds.\n",
      "Extracting and downsampling... 4341  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4341it [00:34, 124.93it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [12794, 19370, 11907, 40053, 2066, 9707, 28990, 14160, 1865, 15528]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:338: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the symbolic link of the video\n",
      "AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to do this manually for including it in the config.yaml file!\n",
      "Videopath: D:\\obstacle_avoidance\\recordings\\071023\\G8CKRT\\oadark\\071023_G8CKRT_whiteblack_Rig2_oadark_top1.avi Coordinates for cropping: None\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071023_G8CKRT_whiteblack_Rig2_oadark_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n",
      "Method  jump  found  3663  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  511.53333333333336 , recorded @  60.0 fps!\n",
      "Overall # of frames:  30692 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 511.53  seconds.\n",
      "Extracting and downsampling... 3663  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3663it [00:29, 126.24it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [8924, 30480, 22473, 13087, 14290, 14838, 9119, 27321, 2234, 29420]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:338: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the symbolic link of the video\n",
      "AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to do this manually for including it in the config.yaml file!\n",
      "Videopath: D:\\obstacle_avoidance\\recordings\\071023\\G8CKRN\\oadark\\071023_G8CKRN_whiteblack_Rig2_oadark_top1.avi Coordinates for cropping: None\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071023_G8CKRN_whiteblack_Rig2_oadark_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n",
      "Method  jump  found  4666  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  714.1833333333333 , recorded @  60.0 fps!\n",
      "Overall # of frames:  42851 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 714.18  seconds.\n",
      "Extracting and downsampling... 4666  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4666it [00:37, 125.53it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [37267, 32805, 18250, 399, 8116, 16157, 3014, 14949, 17689, 18766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:338: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the symbolic link of the video\n",
      "AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to do this manually for including it in the config.yaml file!\n",
      "Videopath: D:\\obstacle_avoidance\\recordings\\071023\\G8CKLN\\oadark\\071023_G8CKLN_whiteblack_Rig2_oadark_top1.avi Coordinates for cropping: None\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071023_G8CKLN_whiteblack_Rig2_oadark_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n",
      "Method  jump  found  4843  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  627.15 , recorded @  60.0 fps!\n",
      "Overall # of frames:  37629 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 627.15  seconds.\n",
      "Extracting and downsampling... 4843  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4843it [00:38, 127.01it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [5974, 33613, 14324, 32859, 17444, 13, 9791, 12425, 8932, 9792]\n",
      "Creating the symbolic link of the video\n",
      "AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to do this manually for including it in the config.yaml file!\n",
      "Videopath: D:\\obstacle_avoidance\\recordings\\071023\\G8CKLT\\oadark\\071023_G8CKLT_whiteblack_Rig2_oadark_top1.avi Coordinates for cropping: None\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071023_G8CKLT_whiteblack_Rig2_oadark_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_outlier_frames(config_path, new_vid_path, automatic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying the videos\n",
      "Copying the videos\n",
      "Copying the videos\n",
      "Copying the videos\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.add_new_videos(config_path, new_vid_path, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking labels if they are outside the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\gui\\refinement.py:735: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  self.Dataframe.columns.set_levels(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking labels if they are outside the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\gui\\refinement.py:735: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  self.Dataframe.columns.set_levels(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking labels if they are outside the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\gui\\refinement.py:735: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  self.Dataframe.columns.set_levels(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking labels if they are outside the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\gui\\refinement.py:735: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  self.Dataframe.columns.set_levels(\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.refine_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 4.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.merge_datasets(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([196, 176,  62, 184, 136,   5,  76, 110,  17,  77, 105, 109, 161,\n",
       "           51, 155,  35, 117, 209,  71, 190,  16, 188, 179,  14, 143,  83,\n",
       "          194, 146, 156,  52,   7,  50, 186, 107, 164, 142,   2,  69, 174,\n",
       "           10,  53, 125, 144,  65, 158, 120,  20, 185, 205,  67, 145,  89,\n",
       "          147, 131, 138,  23, 183, 175,  33, 169, 177, 159,  88, 154, 126,\n",
       "          200,  64, 189, 208, 165, 114,  82, 133, 207,  12,  46, 182,  40,\n",
       "           57,  37, 180,  42, 191, 151,  97, 198,  92, 102,  48, 152,  80,\n",
       "           78,  99, 108,  75, 192,  28,  96, 139, 140, 127,  63, 103, 112,\n",
       "           58,  22,  27, 123,  25,  61,  13, 187, 115, 106,  74,   6, 172,\n",
       "            9,  85, 203,  55, 118, 150, 202,  98,  26,  70, 206,  59, 149,\n",
       "           72,   0,  81,  19,  15, 153, 100,  11,   3, 104, 121,  38, 135,\n",
       "          181, 160, 173, 113, 129, 167, 204,  95, 193,   4, 134,  94, 137,\n",
       "          197, 111,  86,  56,  73,  18,  39,  21,  45, 148,  66,  30,  68,\n",
       "          119,  43,  54, 124,  29, 162, 201,  90,  32, 166, 163,  47, 170,\n",
       "          178, 116,  79, 157, 195, 128,  34, 168, 132,  87,  60,  49, 171,\n",
       "           24,  84,  93,  44]),\n",
       "   array([  8, 101, 199,   1,  31, 130,  91, 141,  41, 122,  36])))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0],\n",
      "                [1],\n",
      "                [2],\n",
      "                [3],\n",
      "                [4],\n",
      "                [5],\n",
      "                [6],\n",
      "                [7],\n",
      "                [8],\n",
      "                [9],\n",
      "                [10],\n",
      "                [11],\n",
      "                [12],\n",
      "                [13],\n",
      "                [14],\n",
      "                [15],\n",
      "                [16],\n",
      "                [17],\n",
      "                [18],\n",
      "                [19]],\n",
      " 'all_joints_names': ['nose',\n",
      "                      'leftear',\n",
      "                      'rightear',\n",
      "                      'spine',\n",
      "                      'midspine',\n",
      "                      'tailbase',\n",
      "                      'midtail',\n",
      "                      'tailend',\n",
      "                      'arenaTL',\n",
      "                      'arenaTR',\n",
      "                      'arenaBL',\n",
      "                      'arenaBR',\n",
      "                      'obstacleTL',\n",
      "                      'obstacleTR',\n",
      "                      'obstacleBR',\n",
      "                      'obstacleBL',\n",
      "                      'leftportT',\n",
      "                      'leftportB',\n",
      "                      'rightportT',\n",
      "                      'rightportB'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'clahe': True,\n",
      " 'claheratio': 0.1,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-4\\\\UnaugmentedDataSet_project_nameApr28\\\\project_name_Mike95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'edge': False,\n",
      " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'histeq': True,\n",
      " 'histeqratio': 0.1,\n",
      " 'init_weights': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-3\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot-2160000',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-4\\\\UnaugmentedDataSet_project_nameApr28\\\\Documentation_data-project_name_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 20,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'sharpen': False,\n",
      " 'sharpenratio': 0.3,\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-4\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already trained DLC with backbone: resnet_50\n",
      "Display_iters overwritten as 1000\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-4\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]], 'all_joints_names': ['nose', 'leftear', 'rightear', 'spine', 'midspine', 'tailbase', 'midtail', 'tailend', 'arenaTL', 'arenaTR', 'arenaBL', 'arenaBR', 'obstacleTL', 'obstacleTR', 'obstacleBR', 'obstacleBL', 'leftportT', 'leftportB', 'rightportT', 'rightportB'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-4\\\\UnaugmentedDataSet_project_nameApr28\\\\project_name_Mike95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-3\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot-2160000', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-4\\\\UnaugmentedDataSet_project_nameApr28\\\\Documentation_data-project_name_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 20, 'pos_dist_thresh': 17, 'project_path': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 2161000 loss: 0.0021 lr: 0.005\n",
      "iteration: 2162000 loss: 0.0020 lr: 0.005\n",
      "iteration: 2163000 loss: 0.0018 lr: 0.005\n",
      "iteration: 2164000 loss: 0.0018 lr: 0.005\n",
      "iteration: 2165000 loss: 0.0017 lr: 0.005\n",
      "iteration: 2166000 loss: 0.0018 lr: 0.005\n",
      "iteration: 2167000 loss: 0.0017 lr: 0.005\n",
      "iteration: 2168000 loss: 0.0017 lr: 0.005\n",
      "iteration: 2169000 loss: 0.0017 lr: 0.005\n",
      "iteration: 2170000 loss: 0.0017 lr: 0.005\n",
      "iteration: 2171000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2172000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2173000 loss: 0.0018 lr: 0.02\n",
      "iteration: 2174000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2175000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2176000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2177000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2178000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2179000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2180000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2181000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2182000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2183000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2184000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2185000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2186000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2187000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2188000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2189000 loss: 0.0017 lr: 0.02\n",
      "iteration: 2190000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2191000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2192000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2193000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2194000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2195000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2196000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2197000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2198000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2199000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2200000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2201000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2202000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2203000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2204000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2205000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2206000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2207000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2208000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2209000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2210000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2211000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2212000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2213000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2214000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2215000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2216000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2217000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2218000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2219000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2220000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2221000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2222000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2223000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2224000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2225000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2226000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2227000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2228000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2229000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2230000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2231000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2232000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2233000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2234000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2235000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2236000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2237000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2238000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2239000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2240000 loss: 0.0016 lr: 0.02\n",
      "iteration: 2241000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2242000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2243000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2244000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2245000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2246000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2247000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2248000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2249000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2250000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2251000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2252000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2253000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2254000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2255000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2256000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2257000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2258000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2259000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2260000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2261000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2262000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2263000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2264000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2265000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2266000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2267000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2268000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2269000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2270000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2271000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2272000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2273000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2274000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2275000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2276000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2277000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2278000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2279000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2280000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2281000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2282000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2283000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2284000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2285000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2286000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2287000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2288000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2289000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2290000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2291000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2292000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2293000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2294000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2295000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2296000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2297000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2298000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2299000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2300000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2301000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2302000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2303000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2304000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2305000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2306000 loss: 0.0015 lr: 0.02\n",
      "iteration: 2307000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2308000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2309000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2310000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2311000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2312000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2313000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2314000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2315000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2316000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2317000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2318000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2319000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2320000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2321000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2322000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2323000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2324000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2325000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2326000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2327000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2328000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2329000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2330000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2331000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2332000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2333000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2334000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2335000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2336000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2337000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2338000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2339000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2340000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2341000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2342000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2343000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2344000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2345000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2346000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2347000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2348000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2349000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2350000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2351000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2352000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2353000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2354000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2355000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2356000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2357000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2358000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2359000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2360000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2361000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2362000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2363000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2364000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2365000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2366000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2367000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2368000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2369000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2370000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2371000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2372000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2373000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2374000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2375000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2376000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2377000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2378000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2379000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2380000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2381000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2382000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2383000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2384000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2385000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2386000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2387000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2388000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2389000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2390000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2391000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2392000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2393000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2394000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2395000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2396000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2397000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2398000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2399000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2400000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2401000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2402000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2403000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2404000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2405000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2406000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2407000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2408000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2409000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2410000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2411000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2412000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2413000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2414000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2415000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2416000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2417000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2418000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2419000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2420000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2421000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2422000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2423000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2424000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2425000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2426000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2427000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2428000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2429000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2430000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2431000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2432000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2433000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2434000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2435000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2436000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2437000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2438000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2439000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2440000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2441000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2442000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2443000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2444000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2445000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2446000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2447000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2448000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2449000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2450000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2451000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2452000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2453000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2454000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2455000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2456000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2457000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2458000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2459000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2460000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2461000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2462000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2463000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2464000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2465000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2466000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2467000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2468000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2469000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2470000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2471000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2472000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2473000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2474000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2475000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2476000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2477000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2478000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2479000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2480000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2481000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2482000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2483000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2484000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2485000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2486000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2487000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2488000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2489000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2490000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2491000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2492000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2493000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2494000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2495000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2496000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2497000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2498000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2499000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2500000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2501000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2502000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2503000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2504000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2505000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2506000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2507000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2508000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2509000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2510000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2511000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2512000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2513000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2514000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2515000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2516000 loss: 0.0014 lr: 0.02\n",
      "iteration: 2517000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2518000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2519000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2520000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2521000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2522000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2523000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2524000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2525000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2526000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2527000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2528000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2529000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2530000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2531000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2532000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2533000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2534000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2535000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2536000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2537000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2538000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2539000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2540000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2541000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2542000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2543000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2544000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2545000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2546000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2547000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2548000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2549000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2550000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2551000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2552000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2553000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2554000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2555000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2556000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2557000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2558000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2559000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2560000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2561000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2562000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2563000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2564000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2565000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2566000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2567000 loss: 0.0012 lr: 0.02\n",
      "iteration: 2568000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2569000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2570000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2571000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2572000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2573000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2574000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2575000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2576000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2577000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2578000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2579000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2580000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2581000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2582000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2583000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2584000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2585000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2586000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2587000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2588000 loss: 0.0012 lr: 0.02\n",
      "iteration: 2589000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2590000 loss: 0.0013 lr: 0.02\n",
      "iteration: 2591000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2592000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2593000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2594000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2595000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2596000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2597000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2598000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2599000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2600000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2601000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2602000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2603000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2604000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2605000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2606000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2607000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2608000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2609000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2610000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2611000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2612000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2613000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2614000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2615000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2616000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2617000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2618000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2619000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2620000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2621000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2622000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2623000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2624000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2625000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2626000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2627000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2628000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2629000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2630000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2631000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2632000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2633000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2634000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2635000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2636000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2637000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2638000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2639000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2640000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2641000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2642000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2643000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2644000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2645000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2646000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2647000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2648000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2649000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2650000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2651000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2652000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2653000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2654000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2655000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2656000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2657000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2658000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2659000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2660000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2661000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2662000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2663000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2664000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2665000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2666000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2667000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2668000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2669000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2670000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2671000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2672000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2673000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2674000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2675000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2676000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2677000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2678000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2679000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2680000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2681000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2682000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2683000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2684000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2685000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2686000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2687000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2688000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2689000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2690000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2691000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2692000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2693000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2694000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2695000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2696000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2697000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2698000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2699000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2700000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2701000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2702000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2703000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2704000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2705000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2706000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2707000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2708000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2709000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2710000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2711000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2712000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2713000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2714000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2715000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2716000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2717000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2718000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2719000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2720000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2721000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2722000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2723000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2724000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2725000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2726000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2727000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2728000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2729000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2730000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2731000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2732000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2733000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2734000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2735000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2736000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2737000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2738000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2739000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2740000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2741000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2742000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2743000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2744000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2745000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2746000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2747000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2748000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2749000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2750000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2751000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2752000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2753000 loss: 0.0012 lr: 0.002\n",
      "iteration: 2754000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2755000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2756000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2757000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2758000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2759000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2760000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2761000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2762000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2763000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2764000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2765000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2766000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2767000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2768000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2769000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2770000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2771000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2772000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2773000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2774000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2775000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2776000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2777000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2778000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2779000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2780000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2781000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2782000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2783000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2784000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2785000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2786000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2787000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2788000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2789000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2790000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2791000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2792000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2793000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2794000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2795000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2796000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2797000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2798000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2799000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2800000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2801000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2802000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2803000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2804000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2805000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2806000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2807000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2808000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2809000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2810000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2811000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2812000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2813000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2814000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2815000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2816000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2817000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2818000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2819000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2820000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2821000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2822000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2823000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2824000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2825000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2826000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2827000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2828000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2829000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2830000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2831000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2832000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2833000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2834000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2835000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2836000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2837000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2838000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2839000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2840000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2841000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2842000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2843000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2844000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2845000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2846000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2847000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2848000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2849000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2850000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2851000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2852000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2853000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2854000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2855000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2856000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2857000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2858000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2859000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2860000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2861000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2862000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2863000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2864000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2865000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2866000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2867000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2868000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2869000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2870000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2871000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2872000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2873000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2874000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2875000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2876000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2877000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2878000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2879000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2880000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2881000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2882000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2883000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2884000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2885000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2886000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2887000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2888000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2889000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2890000 loss: 0.0011 lr: 0.002\n",
      "iteration: 2891000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2892000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2893000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2894000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2895000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2896000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2897000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2898000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2899000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2900000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2901000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2902000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2903000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2904000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2905000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2906000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2907000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2908000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2909000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2910000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2911000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2912000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2913000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2914000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2915000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2916000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2917000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2918000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2919000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2920000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2921000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2922000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2923000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2924000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2925000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2926000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2927000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2928000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2929000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2930000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2931000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2932000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2933000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2934000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2935000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2936000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2937000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2938000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2939000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2940000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2941000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2942000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2943000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2944000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2945000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2946000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2947000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2948000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2949000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2950000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2951000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2952000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2953000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2954000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2955000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2956000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2957000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2958000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2959000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2960000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2961000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2962000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2963000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2964000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2965000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2966000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2967000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2968000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2969000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2970000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2971000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2972000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2973000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2974000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2975000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2976000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2977000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2978000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2979000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2980000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2981000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2982000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2983000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2984000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2985000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2986000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2987000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2988000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2989000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2990000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2991000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2992000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2993000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2994000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2995000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2996000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2997000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2998000 loss: 0.0011 lr: 0.001\n",
      "iteration: 2999000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3000000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3001000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3002000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3003000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3004000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3005000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3006000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3007000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3008000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3009000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3010000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3011000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3012000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3013000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3014000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3015000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3016000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3017000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3018000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3019000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3020000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3021000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3022000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3023000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3024000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3025000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3026000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3027000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3028000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3029000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3030000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3031000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3032000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3033000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3034000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3035000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3036000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3037000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3038000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3039000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3040000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3041000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3042000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3043000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3044000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3045000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3046000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3047000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3048000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3049000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3050000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3051000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3052000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3053000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3054000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3055000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3056000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3057000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3058000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3059000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3060000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3061000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3062000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3063000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3064000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3065000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3066000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3067000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3068000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3069000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3070000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3071000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3072000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3073000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3074000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3075000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3076000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3077000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3078000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3079000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3080000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3081000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3082000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3083000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3084000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3085000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3086000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3087000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3088000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3089000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3090000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3091000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3092000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3093000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3094000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3095000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3096000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3097000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3098000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3099000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3100000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3101000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3102000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3103000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3104000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3105000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3106000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3107000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3108000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3109000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3110000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3111000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3112000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3113000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3114000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3115000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3116000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3117000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3118000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3119000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3120000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3121000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3122000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3123000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3124000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3125000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3126000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3127000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3128000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3129000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3130000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3131000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3132000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3133000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3134000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3135000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3136000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3137000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3138000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3139000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3140000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3141000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3142000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3143000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3144000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3145000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3146000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3147000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3148000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3149000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3150000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3151000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3152000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3153000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3154000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3155000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3156000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3157000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3158000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3159000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3160000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3161000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3162000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3163000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3164000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3165000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3166000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3167000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3168000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3169000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3170000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3171000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3172000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3173000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3174000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3175000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3176000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3177000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3178000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3179000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3180000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3181000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3182000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3183000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3184000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3185000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3186000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3187000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3188000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3189000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3190000 loss: 0.0011 lr: 0.001\n",
      "Exception in thread Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 85, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1116, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, displayiters=1000, saveiters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\videos\\021723_J620RT_control_Rig2_oa_top1.avi ?\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=True, crop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Mike.\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\021723_J620RT_control_Rig2_oa_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:02<00:00,  7.52it/s]\n",
      "100%|| 20/20 [00:02<00:00,  6.77it/s]\n",
      "100%|| 20/20 [00:03<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J619RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:02<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:02<00:00,  7.05it/s]\n",
      "100%|| 20/20 [00:02<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J619LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.add_new_videos(config_path, new_videos, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1952000 for model D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_100522-mike-2022-10-05\\dlc-models\\iteration-0\\object_avoidanceoct22-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  686.18 , recorded with  60.0 fps!\n",
      "Overall # of frames:  41171  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41171/41171 [2:09:19<00:00,  5.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  461.95 , recorded with  60.0 fps!\n",
      "Overall # of frames:  27717  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27717/27717 [1:22:53<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  527.0 , recorded with  60.0 fps!\n",
      "Overall # of frames:  31620  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 31620/31620 [1:34:54<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  880.52 , recorded with  60.0 fps!\n",
      "Overall # of frames:  52831  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 52831/52831 [2:38:45<00:00,  5.55it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  790.68 , recorded with  60.0 fps!\n",
      "Overall # of frames:  47441  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 47441/47441 [2:20:44<00:00,  5.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  906.63 , recorded with  60.0 fps!\n",
      "Overall # of frames:  54398  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 54398/54398 [2:41:29<00:00,  5.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  945.55 , recorded with  60.0 fps!\n",
      "Overall # of frames:  56733  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 56733/56733 [2:49:20<00:00,  5.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  737.72 , recorded with  60.0 fps!\n",
      "Overall # of frames:  44263  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 44263/44263 [2:10:58<00:00,  5.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(config_path, new_videos, shuffle=1, save_as_csv=True, videotype='.avi')\n",
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MODE. Choose either 'manual', 'automatic' or 'match'. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?``               for ipython/jupyter notebook for more details.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, new_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_frames in module deeplabcut.generate_training_dataset.frame_extraction:\n",
      "\n",
      "extract_frames(config, mode='automatic', algo='kmeans', crop=False, userfeedback=True, cluster_step=1, cluster_resizewidth=30, cluster_color=False, opencv=True, slider_width=25, config3d=None, extracted_cam=0, videos_list=None)\n",
      "    Extracts frames from the project videos.\n",
      "    \n",
      "    Frames will be extracted from videos listed in the config.yaml file.\n",
      "    \n",
      "    The frames are selected from the videos in a randomly and temporally uniformly\n",
      "    distributed way (``uniform``), by clustering based on visual appearance\n",
      "    (``k-means``), or by manual selection.\n",
      "    \n",
      "    After frames have been extracted from all videos from one camera, matched frames\n",
      "    from other cameras can be extracted using ``mode = \"match\"``. This is necessary if\n",
      "    you plan to use epipolar lines to improve labeling across multiple camera angles.\n",
      "    It will overwrite previously extracted images from the second camera angle if\n",
      "    necessary.\n",
      "    \n",
      "    Please refer to the user guide for more details on methods and parameters\n",
      "    https://www.nature.com/articles/s41596-019-0176-0 or the preprint:\n",
      "    https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    config : string\n",
      "        Full path of the config.yaml file as a string.\n",
      "    \n",
      "    mode : string. Either ``\"automatic\"``, ``\"manual\"`` or ``\"match\"``.\n",
      "        String containing the mode of extraction. It must be either ``\"automatic\"`` or\n",
      "        ``\"manual\"`` to extract the initial set of frames. It can also be ``\"match\"``\n",
      "        to match frames between the cameras in preparation for the use of epipolar line\n",
      "        during labeling; namely, extract from camera_1 first, then run this to extract\n",
      "        the matched frames in camera_2.\n",
      "    \n",
      "        WARNING: if you use ``\"match\"``, and you previously extracted and labeled\n",
      "        frames from the second camera, this will overwrite your data. This will require\n",
      "        you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with\n",
      "        caution!\n",
      "    \n",
      "    algo : string, Either ``\"kmeans\"`` or ``\"uniform\"``, Default: `\"kmeans\"`.\n",
      "        String specifying the algorithm to use for selecting the frames. Currently,\n",
      "        deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag\n",
      "        is only required for ``automatic`` mode and the default is ``kmeans``. For\n",
      "        ``\"uniform\"``, frames are picked in temporally uniform way, ``\"kmeans\"``\n",
      "        performs clustering on downsampled frames (see user guide for details).\n",
      "    \n",
      "        NOTE: Color information is discarded for ``\"kmeans\"``, thus e.g. for\n",
      "        camouflaged octopus clustering one might want to change this.\n",
      "    \n",
      "    crop : bool or str, optional\n",
      "        If ``True``, video frames are cropped according to the corresponding\n",
      "        coordinates stored in the project configuration file. Alternatively, if\n",
      "        cropping coordinates are not known yet, crop=``\"GUI\"`` triggers a user\n",
      "        interface where the cropping area can be manually drawn and saved.\n",
      "    \n",
      "    userfeedback: bool, optional\n",
      "        If this is set to ``False`` during ``\"automatic\"`` mode then frames for all\n",
      "        videos are extracted. The user can set this to ``\"True\"``, which will result in\n",
      "        a dialog, where the user is asked for each video if (additional/any) frames\n",
      "        from this video should be extracted. Use this, e.g. if you have already labeled\n",
      "        some folders and want to extract data for new videos.\n",
      "    \n",
      "    cluster_resizewidth: int, default: 30\n",
      "        For ``\"k-means\"`` one can change the width to which the images are downsampled\n",
      "        (aspect ratio is fixed).\n",
      "    \n",
      "    cluster_step: int, default: 1\n",
      "        By default each frame is used for clustering, but for long videos one could\n",
      "        only use every nth frame (set using this parameter). This saves memory before\n",
      "        clustering can start, however, reading the individual frames takes longer due\n",
      "        to the skipping.\n",
      "    \n",
      "    cluster_color: bool, default: False\n",
      "        If ``\"False\"`` then each downsampled image is treated as a grayscale vector\n",
      "        (discarding color information). If ``\"True\"``, then the color channels are\n",
      "        considered. This increases the computational complexity.\n",
      "    \n",
      "    opencv: bool, default: True\n",
      "        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).\n",
      "    \n",
      "    slider_width: int, default: 25\n",
      "        Width of the video frames slider, in percent of window.\n",
      "    \n",
      "    config3d: string, optional\n",
      "        Path to the project configuration file in the 3D project. This will be used to\n",
      "        match frames extracted from all cameras present in the field 'camera_names' to\n",
      "        the frames extracted from the camera given by the parameter 'extracted_cam'.\n",
      "    \n",
      "    extracted_cam: int, default: 0\n",
      "        The index of the camera that already has extracted frames. This will match\n",
      "        frame numbers to extract for all other cameras. This parameter is necessary if\n",
      "        you wish to use epipolar lines in the labeling toolbox. Only use if\n",
      "        ``mode='match'`` and ``config3d`` is provided.\n",
      "    \n",
      "    videos_list: list[str], Default: None\n",
      "        A list of the string containing full paths to videos to extract frames for. If\n",
      "        this is left as ``None`` all videos specified in the config file will have\n",
      "        frames extracted. Otherwise one can select a subset by passing those paths.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Use the function ``add_new_videos`` at any stage of the project to add new videos\n",
      "    to the config file and extract their frames.\n",
      "    \n",
      "    The following parameters for automatic extraction are used from the config file\n",
      "    \n",
      "    * ``numframes2pick``\n",
      "    * ``start`` and ``stop``\n",
      "    \n",
      "    While selecting the frames manually, you do not need to specify the ``crop``\n",
      "    parameter in the command. Rather, you will get a prompt in the graphic user\n",
      "    interface to choose if you need to crop or not.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    To extract frames automatically with 'kmeans' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            config='/analysis/project/reaching-task/config.yaml',\n",
      "            mode='automatic',\n",
      "            algo='kmeans',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'kmeans' and then defining the cropping area\n",
      "    using a GUI\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            'GUI',\n",
      "        )\n",
      "    \n",
      "    To consider the color information when extracting frames automatically with\n",
      "    'kmeans'\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            cluster_color=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'uniform' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'uniform',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames manually\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual'\n",
      "        )\n",
      "    \n",
      "    To extract frames manually, with a 60% wide frames slider\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,\n",
      "        )\n",
      "    \n",
      "    To extract frames from a second camera that match the frames extracted from the\n",
      "    first\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            mode='match',\n",
      "            extracted_cam=0,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deeplabcut.extract_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18909df720030512de15b5464fd804d554a62031bd4c4e65819719dd1ff43646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
