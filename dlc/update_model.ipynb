{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\__init__.py:78: UserWarning: \n",
      "        As PyTorch is not installed, unsupervised identity learning will not be available.\n",
      "        Please run `pip install torch`, or ignore this warning.\n",
      "        \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\config.yaml\"\n",
    "\n",
    "new_videos = [\n",
    "                \"D:/obstacle_avoidance/recordings/022223\\J620RT\\oa_dark/022223_J620RT_control_Rig2_oa_dark_top1.avi\",\n",
    "                \"D:/obstacle_avoidance/recordings/022223\\J620LT\\oa_dark/022223_J620LT_control_Rig2_oa_dark_top1.avi\",\n",
    "                \"D:/obstacle_avoidance/recordings/022223\\J619RT\\oa_dark/022223_J619RT_control_Rig2_oa_dark_top1.avi\",\n",
    "                \"D:/obstacle_avoidance/recordings/022223\\J619LT\\oa_dark/022223_J619LT_control_Rig2_oa_dark_top1.avi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying the videos\n",
      "Copying the videos\n",
      "Copying the videos\n",
      "Copying the videos\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.add_new_videos(config_path, new_videos, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\videos\\021723_J620RT_control_Rig2_oa_top1.avi ?\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=True, crop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Mike.\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\021723_J620RT_control_Rig2_oa_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.52it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.77it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J619RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.05it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J619LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.add_new_videos(config_path, new_videos, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1952000 for model D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_100522-mike-2022-10-05\\dlc-models\\iteration-0\\object_avoidanceoct22-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  686.18 , recorded with  60.0 fps!\n",
      "Overall # of frames:  41171  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41171/41171 [2:09:19<00:00,  5.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  461.95 , recorded with  60.0 fps!\n",
      "Overall # of frames:  27717  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27717/27717 [1:22:53<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  527.0 , recorded with  60.0 fps!\n",
      "Overall # of frames:  31620  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31620/31620 [1:34:54<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  880.52 , recorded with  60.0 fps!\n",
      "Overall # of frames:  52831  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52831/52831 [2:38:45<00:00,  5.55it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  790.68 , recorded with  60.0 fps!\n",
      "Overall # of frames:  47441  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47441/47441 [2:20:44<00:00,  5.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  906.63 , recorded with  60.0 fps!\n",
      "Overall # of frames:  54398  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54398/54398 [2:41:29<00:00,  5.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  945.55 , recorded with  60.0 fps!\n",
      "Overall # of frames:  56733  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56733/56733 [2:49:20<00:00,  5.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  737.72 , recorded with  60.0 fps!\n",
      "Overall # of frames:  44263  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44263/44263 [2:10:58<00:00,  5.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(config_path, new_videos, shuffle=1, save_as_csv=True, videotype='.avi')\n",
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MODE. Choose either 'manual', 'automatic' or 'match'. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?``               for ipython/jupyter notebook for more details.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, new_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_frames in module deeplabcut.generate_training_dataset.frame_extraction:\n",
      "\n",
      "extract_frames(config, mode='automatic', algo='kmeans', crop=False, userfeedback=True, cluster_step=1, cluster_resizewidth=30, cluster_color=False, opencv=True, slider_width=25, config3d=None, extracted_cam=0, videos_list=None)\n",
      "    Extracts frames from the project videos.\n",
      "    \n",
      "    Frames will be extracted from videos listed in the config.yaml file.\n",
      "    \n",
      "    The frames are selected from the videos in a randomly and temporally uniformly\n",
      "    distributed way (``uniform``), by clustering based on visual appearance\n",
      "    (``k-means``), or by manual selection.\n",
      "    \n",
      "    After frames have been extracted from all videos from one camera, matched frames\n",
      "    from other cameras can be extracted using ``mode = \"match\"``. This is necessary if\n",
      "    you plan to use epipolar lines to improve labeling across multiple camera angles.\n",
      "    It will overwrite previously extracted images from the second camera angle if\n",
      "    necessary.\n",
      "    \n",
      "    Please refer to the user guide for more details on methods and parameters\n",
      "    https://www.nature.com/articles/s41596-019-0176-0 or the preprint:\n",
      "    https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    config : string\n",
      "        Full path of the config.yaml file as a string.\n",
      "    \n",
      "    mode : string. Either ``\"automatic\"``, ``\"manual\"`` or ``\"match\"``.\n",
      "        String containing the mode of extraction. It must be either ``\"automatic\"`` or\n",
      "        ``\"manual\"`` to extract the initial set of frames. It can also be ``\"match\"``\n",
      "        to match frames between the cameras in preparation for the use of epipolar line\n",
      "        during labeling; namely, extract from camera_1 first, then run this to extract\n",
      "        the matched frames in camera_2.\n",
      "    \n",
      "        WARNING: if you use ``\"match\"``, and you previously extracted and labeled\n",
      "        frames from the second camera, this will overwrite your data. This will require\n",
      "        you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with\n",
      "        caution!\n",
      "    \n",
      "    algo : string, Either ``\"kmeans\"`` or ``\"uniform\"``, Default: `\"kmeans\"`.\n",
      "        String specifying the algorithm to use for selecting the frames. Currently,\n",
      "        deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag\n",
      "        is only required for ``automatic`` mode and the default is ``kmeans``. For\n",
      "        ``\"uniform\"``, frames are picked in temporally uniform way, ``\"kmeans\"``\n",
      "        performs clustering on downsampled frames (see user guide for details).\n",
      "    \n",
      "        NOTE: Color information is discarded for ``\"kmeans\"``, thus e.g. for\n",
      "        camouflaged octopus clustering one might want to change this.\n",
      "    \n",
      "    crop : bool or str, optional\n",
      "        If ``True``, video frames are cropped according to the corresponding\n",
      "        coordinates stored in the project configuration file. Alternatively, if\n",
      "        cropping coordinates are not known yet, crop=``\"GUI\"`` triggers a user\n",
      "        interface where the cropping area can be manually drawn and saved.\n",
      "    \n",
      "    userfeedback: bool, optional\n",
      "        If this is set to ``False`` during ``\"automatic\"`` mode then frames for all\n",
      "        videos are extracted. The user can set this to ``\"True\"``, which will result in\n",
      "        a dialog, where the user is asked for each video if (additional/any) frames\n",
      "        from this video should be extracted. Use this, e.g. if you have already labeled\n",
      "        some folders and want to extract data for new videos.\n",
      "    \n",
      "    cluster_resizewidth: int, default: 30\n",
      "        For ``\"k-means\"`` one can change the width to which the images are downsampled\n",
      "        (aspect ratio is fixed).\n",
      "    \n",
      "    cluster_step: int, default: 1\n",
      "        By default each frame is used for clustering, but for long videos one could\n",
      "        only use every nth frame (set using this parameter). This saves memory before\n",
      "        clustering can start, however, reading the individual frames takes longer due\n",
      "        to the skipping.\n",
      "    \n",
      "    cluster_color: bool, default: False\n",
      "        If ``\"False\"`` then each downsampled image is treated as a grayscale vector\n",
      "        (discarding color information). If ``\"True\"``, then the color channels are\n",
      "        considered. This increases the computational complexity.\n",
      "    \n",
      "    opencv: bool, default: True\n",
      "        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).\n",
      "    \n",
      "    slider_width: int, default: 25\n",
      "        Width of the video frames slider, in percent of window.\n",
      "    \n",
      "    config3d: string, optional\n",
      "        Path to the project configuration file in the 3D project. This will be used to\n",
      "        match frames extracted from all cameras present in the field 'camera_names' to\n",
      "        the frames extracted from the camera given by the parameter 'extracted_cam'.\n",
      "    \n",
      "    extracted_cam: int, default: 0\n",
      "        The index of the camera that already has extracted frames. This will match\n",
      "        frame numbers to extract for all other cameras. This parameter is necessary if\n",
      "        you wish to use epipolar lines in the labeling toolbox. Only use if\n",
      "        ``mode='match'`` and ``config3d`` is provided.\n",
      "    \n",
      "    videos_list: list[str], Default: None\n",
      "        A list of the string containing full paths to videos to extract frames for. If\n",
      "        this is left as ``None`` all videos specified in the config file will have\n",
      "        frames extracted. Otherwise one can select a subset by passing those paths.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Use the function ``add_new_videos`` at any stage of the project to add new videos\n",
      "    to the config file and extract their frames.\n",
      "    \n",
      "    The following parameters for automatic extraction are used from the config file\n",
      "    \n",
      "    * ``numframes2pick``\n",
      "    * ``start`` and ``stop``\n",
      "    \n",
      "    While selecting the frames manually, you do not need to specify the ``crop``\n",
      "    parameter in the command. Rather, you will get a prompt in the graphic user\n",
      "    interface to choose if you need to crop or not.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    To extract frames automatically with 'kmeans' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            config='/analysis/project/reaching-task/config.yaml',\n",
      "            mode='automatic',\n",
      "            algo='kmeans',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'kmeans' and then defining the cropping area\n",
      "    using a GUI\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            'GUI',\n",
      "        )\n",
      "    \n",
      "    To consider the color information when extracting frames automatically with\n",
      "    'kmeans'\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            cluster_color=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'uniform' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'uniform',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames manually\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual'\n",
      "        )\n",
      "    \n",
      "    To extract frames manually, with a 60% wide frames slider\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,\n",
      "        )\n",
      "    \n",
      "    To extract frames from a second camera that match the frames extracted from the\n",
      "    first\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            mode='match',\n",
      "            extracted_cam=0,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deeplabcut.extract_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d5aad94d69af8d54a80740ac3e999eae8a40d3534c4b9b883255615339559dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
