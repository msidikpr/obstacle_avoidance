{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "from pathlib import Path\n",
    "## set path to dlc project config file \n",
    "config_path = r\"D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set path to dlc project config file \n",
    "config_path = r\"D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vid_path = [r\"D:\\obstacle_avoidance\\recordings\\071723\\G8CKTT\\oa\\071723_G8CKTT_blackwhite_Rig2_oa_top1.avi\",r\"D:\\obstacle_avoidance\\recordings\\071723\\G8CKRN\\oa\\071723_G8CKRN_blackwhite_Rig2_oa_top1.avi\",\n",
    "                r\"D:\\obstacle_avoidance\\recordings\\081023\\G8CK1RT\\oa\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.avi\",r\"D:\\obstacle_avoidance\\recordings\\081023\\G8CK1LT\\oa\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.avi\",\n",
    "                r\"D:\\obstacle_avoidance\\recordings\\071823\\G8CKLT\\oa\\071823_G8CKLT_noisewhite_Rig2_oa_top1.avi\",r\"D:\\obstacle_avoidance\\recordings\\071823\\G8CKRT\\oa\\071823_G8CKRT_noisewhite_Rig2_oa_top1.avi\"\n",
    "                \n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-3190000 for model D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\dlc-models\\iteration-4\\project_nameApr28-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071723\\G8CKTT\\oa\\071723_G8CKTT_blackwhite_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\071723\\G8CKTT\\oa\\071723_G8CKTT_blackwhite_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  613.93 , recorded with  60.0 fps!\n",
      "Overall # of frames:  36836  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36836/36836 [08:20<00:00, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\071723\\G8CKTT\\oa...\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071723\\G8CKRN\\oa\\071723_G8CKRN_blackwhite_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\071723\\G8CKRN\\oa\\071723_G8CKRN_blackwhite_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  517.77 , recorded with  60.0 fps!\n",
      "Overall # of frames:  31066  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31066/31066 [06:45<00:00, 76.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\071723\\G8CKRN\\oa...\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\081023\\G8CK1RT\\oa\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\081023\\G8CK1RT\\oa\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  511.15 , recorded with  60.0 fps!\n",
      "Overall # of frames:  30669  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30669/30669 [06:44<00:00, 75.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\081023\\G8CK1RT\\oa...\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\081023\\G8CK1LT\\oa\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\081023\\G8CK1LT\\oa\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  483.62 , recorded with  60.0 fps!\n",
      "Overall # of frames:  29017  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29017/29017 [06:40<00:00, 72.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\081023\\G8CK1LT\\oa...\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071823\\G8CKLT\\oa\\071823_G8CKLT_noisewhite_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\071823\\G8CKLT\\oa\\071823_G8CKLT_noisewhite_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  502.95 , recorded with  60.0 fps!\n",
      "Overall # of frames:  30177  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30177/30177 [06:58<00:00, 72.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\071823\\G8CKLT\\oa...\n",
      "Starting to analyze %  D:\\obstacle_avoidance\\recordings\\071823\\G8CKRT\\oa\\071823_G8CKRT_noisewhite_Rig2_oa_top1.avi\n",
      "Loading  D:\\obstacle_avoidance\\recordings\\071823\\G8CKRT\\oa\\071823_G8CKRT_noisewhite_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  319.28 , recorded with  60.0 fps!\n",
      "Overall # of frames:  19157  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19157/19157 [04:21<00:00, 73.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\071823\\G8CKRT\\oa...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DLC_resnet50_project_nameApr28shuffle1_3190000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## analyze new videos\n",
    "deeplabcut.analyze_videos(config_path, new_vid_path, shuffle=1, save_as_csv=False, videotype='.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  3733  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  613.9333333333333 , recorded @  60.0 fps!\n",
      "Overall # of frames:  36836 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 613.93  seconds.\n",
      "Extracting and downsampling... 3733  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3733it [00:29, 124.97it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [34271, 25183, 1396, 36699, 17475, 28416, 35343, 21344, 8302, 31330]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\071723\\G8CKTT\\oa\\071723_G8CKTT_blackwhite_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\071723_G8CKTT_blackwhite_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071723_G8CKTT_blackwhite_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  2886  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  517.7666666666667 , recorded @  60.0 fps!\n",
      "Overall # of frames:  31066 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 517.77  seconds.\n",
      "Extracting and downsampling... 2886  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2886it [00:23, 122.28it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [20738, 30652, 16760, 8701, 27014, 8106, 24378, 23280, 20003, 11453]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\071723\\G8CKRN\\oa\\071723_G8CKRN_blackwhite_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\071723_G8CKRN_blackwhite_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071723_G8CKRN_blackwhite_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  6374  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  511.15 , recorded @  60.0 fps!\n",
      "Overall # of frames:  30669 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 511.15  seconds.\n",
      "Extracting and downsampling... 6374  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6374it [00:50, 125.16it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [17507, 30397, 63, 1752, 4017, 16301, 6839, 28592, 8722, 15216]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\081023\\G8CK1RT\\oa\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\081023_G8CK1RT_noisenoise_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  3728  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  483.6166666666667 , recorded @  60.0 fps!\n",
      "Overall # of frames:  29017 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 483.62  seconds.\n",
      "Extracting and downsampling... 3728  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3728it [00:30, 120.45it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [11676, 23781, 16385, 7272, 583, 28136, 16894, 7845, 3578, 10341]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\081023\\G8CK1LT\\oa\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\081023_G8CK1LT_noisenoise_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  2214  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  502.95 , recorded @  60.0 fps!\n",
      "Overall # of frames:  30177 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 502.95  seconds.\n",
      "Extracting and downsampling... 2214  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2214it [00:18, 122.64it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [11650, 4422, 7285, 4884, 10064, 9292, 710, 8131, 15722, 10495]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\071823\\G8CKLT\\oa\\071823_G8CKLT_noisewhite_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\071823_G8CKLT_noisewhite_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071823_G8CKLT_noisewhite_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\refine_training_dataset\\outlier_frames.py:403: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  sum_ = temp_dt.sum(axis=1, level=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  jump  found  2164  putative outlier frames.\n",
      "Do you want to proceed with extracting  10  of those?\n",
      "If this list is very large, perhaps consider changing the parameters (start, stop, p_bound, comparisonbodyparts) or use a different method.\n",
      "Loading video...\n",
      "Duration of video [s]:  319.28333333333336 , recorded @  60.0 fps!\n",
      "Overall # of frames:  19157 with (cropped) frame dimensions: \n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 319.28  seconds.\n",
      "Extracting and downsampling... 2164  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2164it [00:17, 126.71it/s]\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Let's select frames indices: [11213, 219, 4595, 10433, 7387, 2363, 1112, 16709, 2377, 3298]\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Symlink creation impossible (exFat architecture?): cutting/pasting the video instead.\n",
      "D:\\obstacle_avoidance\\recordings\\071823\\G8CKRT\\oa\\071823_G8CKRT_noisewhite_Rig2_oa_top1.avi moved to D:\\obstacle_avoidance\\deeplabcut\\project_name-Mike-2023-04-28\\videos\\071823_G8CKRT_noisewhite_Rig2_oa_top1.avi\n",
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n",
      "The outlier frames are extracted. They are stored in the subdirectory labeled-data\\071823_G8CKRT_noisewhite_Rig2_oa_top1.\n",
      "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_outlier_frames(config_path, new_vid_path, automatic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"change to ephys 2\"\"\"\n",
    "deeplabcut.add_new_videos(config_path, new_vid_path, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking labels if they are outside the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys0\\lib\\site-packages\\deeplabcut\\gui\\refinement.py:735: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  self.Dataframe.columns.set_levels(\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.refine_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 5.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.merge_datasets(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 64, 135, 153, 189, 253, 198, 144, 180, 256,  15, 244, 139, 124,\n",
       "          103,   8, 166,  45, 199,  59, 264, 176,  55,   7,  76, 252,  83,\n",
       "          190, 136, 188, 101,  96,  74, 261, 215, 227, 226,  92, 159, 219,\n",
       "          182, 116,  27, 154, 230, 236,  37, 187, 134,  20, 240, 181, 207,\n",
       "          206, 146,   5,  22, 196, 110,  12, 201,  63, 179,  90, 111,  73,\n",
       "           44, 200, 171, 158, 125, 106,  18, 137, 255,  89,  71, 108, 222,\n",
       "          259, 126,  33,  97, 217, 232, 250,  21, 234, 161,  16, 118,  75,\n",
       "          194, 210, 145, 109,   4, 150,  61,  67,  52,  66,  26, 205, 212,\n",
       "          122, 248,  40,  13, 107, 239, 265,   3, 129,  24,  30, 249, 157,\n",
       "           60,  56, 245, 155,  46,  19, 209, 221,  54, 224,  80,  51,   2,\n",
       "          228, 104, 173,  86,  10, 160,  58,  41,  14,  50, 191, 123,  62,\n",
       "          156, 184, 130, 269, 152,  43, 218, 138, 162, 178, 149, 112, 204,\n",
       "           98, 235,  93, 168,  36, 113,   0,  94,  95, 220,  69,  49,  48,\n",
       "           85, 267, 141,  23, 246, 143,  78, 100, 131, 225, 268,   6,  68,\n",
       "           84, 170, 121, 140, 262, 237, 231, 213,  91, 238, 254,  11, 119,\n",
       "          102,  35,  57, 169,  65,   1, 120, 223, 186,  42, 105, 132, 241,\n",
       "           17,  38, 133,  53, 164, 214, 128,  34,  28, 183, 114, 203, 163,\n",
       "          151, 202,  31, 229, 127, 185, 247, 257,  32, 167, 142, 233, 147,\n",
       "           29, 177, 216,  99,  82, 266, 175,  79, 197, 243, 208, 115, 148,\n",
       "          263,  72,  77,  25, 165,  81, 258, 174, 260]),\n",
       "   array([ 39, 193,  88,  70,  87, 242, 211,   9, 195, 251, 192, 117,  47,\n",
       "          172])))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0],\n",
      "                [1],\n",
      "                [2],\n",
      "                [3],\n",
      "                [4],\n",
      "                [5],\n",
      "                [6],\n",
      "                [7],\n",
      "                [8],\n",
      "                [9],\n",
      "                [10],\n",
      "                [11],\n",
      "                [12],\n",
      "                [13],\n",
      "                [14],\n",
      "                [15],\n",
      "                [16],\n",
      "                [17],\n",
      "                [18],\n",
      "                [19]],\n",
      " 'all_joints_names': ['nose',\n",
      "                      'leftear',\n",
      "                      'rightear',\n",
      "                      'spine',\n",
      "                      'midspine',\n",
      "                      'tailbase',\n",
      "                      'midtail',\n",
      "                      'tailend',\n",
      "                      'arenaTL',\n",
      "                      'arenaTR',\n",
      "                      'arenaBL',\n",
      "                      'arenaBR',\n",
      "                      'obstacleTL',\n",
      "                      'obstacleTR',\n",
      "                      'obstacleBR',\n",
      "                      'obstacleBL',\n",
      "                      'leftportT',\n",
      "                      'leftportB',\n",
      "                      'rightportT',\n",
      "                      'rightportB'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'clahe': True,\n",
      " 'claheratio': 0.1,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-5\\\\UnaugmentedDataSet_project_nameApr28\\\\project_name_Mike95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'edge': False,\n",
      " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'histeq': True,\n",
      " 'histeqratio': 0.1,\n",
      " 'init_weights': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-4\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot-3190000',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-5\\\\UnaugmentedDataSet_project_nameApr28\\\\Documentation_data-project_name_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 20,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'sharpen': False,\n",
      " 'sharpenratio': 0.3,\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-5\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n",
      "Loading already trained DLC with backbone: resnet_50\n",
      "Display_iters overwritten as 1000\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-5\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]], 'all_joints_names': ['nose', 'leftear', 'rightear', 'spine', 'midspine', 'tailbase', 'midtail', 'tailend', 'arenaTL', 'arenaTR', 'arenaBL', 'arenaBR', 'obstacleTL', 'obstacleTR', 'obstacleBR', 'obstacleBL', 'leftportT', 'leftportB', 'rightportT', 'rightportB'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-5\\\\UnaugmentedDataSet_project_nameApr28\\\\project_name_Mike95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28\\\\dlc-models\\\\iteration-4\\\\project_nameApr28-trainset95shuffle1\\\\train\\\\snapshot-3190000', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-5\\\\UnaugmentedDataSet_project_nameApr28\\\\Documentation_data-project_name_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 20, 'pos_dist_thresh': 17, 'project_path': 'D:\\\\obstacle_avoidance\\\\deeplabcut\\\\project_name-Mike-2023-04-28', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 3191000 loss: 0.0018 lr: 0.005\n",
      "iteration: 3192000 loss: 0.0017 lr: 0.005\n",
      "iteration: 3193000 loss: 0.0017 lr: 0.005\n",
      "iteration: 3194000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3195000 loss: 0.0016 lr: 0.005\n",
      "iteration: 3196000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3197000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3198000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3199000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3200000 loss: 0.0015 lr: 0.005\n",
      "iteration: 3201000 loss: 0.0016 lr: 0.02\n",
      "iteration: 3202000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3203000 loss: 0.0016 lr: 0.02\n",
      "iteration: 3204000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3205000 loss: 0.0016 lr: 0.02\n",
      "iteration: 3206000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3207000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3208000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3209000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3210000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3211000 loss: 0.0016 lr: 0.02\n",
      "iteration: 3212000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3213000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3214000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3215000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3216000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3217000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3218000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3219000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3220000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3221000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3222000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3223000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3224000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3225000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3226000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3227000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3228000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3229000 loss: 0.0015 lr: 0.02\n",
      "iteration: 3230000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3231000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3232000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3233000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3234000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3235000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3236000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3237000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3238000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3239000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3240000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3241000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3242000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3243000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3244000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3245000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3246000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3247000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3248000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3249000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3250000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3251000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3252000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3253000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3254000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3255000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3256000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3257000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3258000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3259000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3260000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3261000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3262000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3263000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3264000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3265000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3266000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3267000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3268000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3269000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3270000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3271000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3272000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3273000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3274000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3275000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3276000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3277000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3278000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3279000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3280000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3281000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3282000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3283000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3284000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3285000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3286000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3287000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3288000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3289000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3290000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3291000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3292000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3293000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3294000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3295000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3296000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3297000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3298000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3299000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3300000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3301000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3302000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3303000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3304000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3305000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3306000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3307000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3308000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3309000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3310000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3311000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3312000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3313000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3314000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3315000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3316000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3317000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3318000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3319000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3320000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3321000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3322000 loss: 0.0014 lr: 0.02\n",
      "iteration: 3323000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3324000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3325000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3326000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3327000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3328000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3329000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3330000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3331000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3332000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3333000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3334000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3335000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3336000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3337000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3338000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3339000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3340000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3341000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3342000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3343000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3344000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3345000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3346000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3347000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3348000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3349000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3350000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3351000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3352000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3353000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3354000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3355000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3356000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3357000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3358000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3359000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3360000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3361000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3362000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3363000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3364000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3365000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3366000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3367000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3368000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3369000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3370000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3371000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3372000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3373000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3374000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3375000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3376000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3377000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3378000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3379000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3380000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3381000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3382000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3383000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3384000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3385000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3386000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3387000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3388000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3389000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3390000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3391000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3392000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3393000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3394000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3395000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3396000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3397000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3398000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3399000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3400000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3401000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3402000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3403000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3404000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3405000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3406000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3407000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3408000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3409000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3410000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3411000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3412000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3413000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3414000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3415000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3416000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3417000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3418000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3419000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3420000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3421000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3422000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3423000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3424000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3425000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3426000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3427000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3428000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3429000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3430000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3431000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3432000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3433000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3434000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3435000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3436000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3437000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3438000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3439000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3440000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3441000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3442000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3443000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3444000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3445000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3446000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3447000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3448000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3449000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3450000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3451000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3452000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3453000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3454000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3455000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3456000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3457000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3458000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3459000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3460000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3461000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3462000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3463000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3464000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3465000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3466000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3467000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3468000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3469000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3470000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3471000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3472000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3473000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3474000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3475000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3476000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3477000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3478000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3479000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3480000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3481000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3482000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3483000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3484000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3485000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3486000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3487000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3488000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3489000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3490000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3491000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3492000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3493000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3494000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3495000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3496000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3497000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3498000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3499000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3500000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3501000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3502000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3503000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3504000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3505000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3506000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3507000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3508000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3509000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3510000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3511000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3512000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3513000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3514000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3515000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3516000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3517000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3518000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3519000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3520000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3521000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3522000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3523000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3524000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3525000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3526000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3527000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3528000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3529000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3530000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3531000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3532000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3533000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3534000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3535000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3536000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3537000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3538000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3539000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3540000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3541000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3542000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3543000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3544000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3545000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3546000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3547000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3548000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3549000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3550000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3551000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3552000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3553000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3554000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3555000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3556000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3557000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3558000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3559000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3560000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3561000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3562000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3563000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3564000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3565000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3566000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3567000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3568000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3569000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3570000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3571000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3572000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3573000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3574000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3575000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3576000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3577000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3578000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3579000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3580000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3581000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3582000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3583000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3584000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3585000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3586000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3587000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3588000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3589000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3590000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3591000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3592000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3593000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3594000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3595000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3596000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3597000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3598000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3599000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3600000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3601000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3602000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3603000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3604000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3605000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3606000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3607000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3608000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3609000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3610000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3611000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3612000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3613000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3614000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3615000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3616000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3617000 loss: 0.0013 lr: 0.02\n",
      "iteration: 3618000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3619000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3620000 loss: 0.0012 lr: 0.02\n",
      "iteration: 3621000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3622000 loss: 0.0012 lr: 0.002\n",
      "iteration: 3623000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3624000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3625000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3626000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3627000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3628000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3629000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3630000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3631000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3632000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3633000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3634000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3635000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3636000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3637000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3638000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3639000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3640000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3641000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3642000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3643000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3644000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3645000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3646000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3647000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3648000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3649000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3650000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3651000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3652000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3653000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3654000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3655000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3656000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3657000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3658000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3659000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3660000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3661000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3662000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3663000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3664000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3665000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3666000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3667000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3668000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3669000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3670000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3671000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3672000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3673000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3674000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3675000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3676000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3677000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3678000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3679000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3680000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3681000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3682000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3683000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3684000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3685000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3686000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3687000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3688000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3689000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3690000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3691000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3692000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3693000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3694000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3695000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3696000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3697000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3698000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3699000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3700000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3701000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3702000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3703000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3704000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3705000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3706000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3707000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3708000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3709000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3710000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3711000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3712000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3713000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3714000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3715000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3716000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3717000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3718000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3719000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3720000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3721000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3722000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3723000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3724000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3725000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3726000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3727000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3728000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3729000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3730000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3731000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3732000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3733000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3734000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3735000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3736000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3737000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3738000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3739000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3740000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3741000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3742000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3743000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3744000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3745000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3746000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3747000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3748000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3749000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3750000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3751000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3752000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3753000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3754000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3755000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3756000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3757000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3758000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3759000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3760000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3761000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3762000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3763000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3764000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3765000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3766000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3767000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3768000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3769000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3770000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3771000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3772000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3773000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3774000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3775000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3776000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3777000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3778000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3779000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3780000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3781000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3782000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3783000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3784000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3785000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3786000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3787000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3788000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3789000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3790000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3791000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3792000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3793000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3794000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3795000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3796000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3797000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3798000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3799000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3800000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3801000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3802000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3803000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3804000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3805000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3806000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3807000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3808000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3809000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3810000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3811000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3812000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3813000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3814000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3815000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3816000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3817000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3818000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3819000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3820000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3821000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3822000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3823000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3824000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3825000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3826000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3827000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3828000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3829000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3830000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3831000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3832000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3833000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3834000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3835000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3836000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3837000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3838000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3839000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3840000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3841000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3842000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3843000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3844000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3845000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3846000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3847000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3848000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3849000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3850000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3851000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3852000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3853000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3854000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3855000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3856000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3857000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3858000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3859000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3860000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3861000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3862000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3863000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3864000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3865000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3866000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3867000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3868000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3869000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3870000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3871000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3872000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3873000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3874000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3875000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3876000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3877000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3878000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3879000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3880000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3881000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3882000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3883000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3884000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3885000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3886000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3887000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3888000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3889000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3890000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3891000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3892000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3893000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3894000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3895000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3896000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3897000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3898000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3899000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3900000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3901000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3902000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3903000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3904000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3905000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3906000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3907000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3908000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3909000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3910000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3911000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3912000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3913000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3914000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3915000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3916000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3917000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3918000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3919000 loss: 0.0011 lr: 0.002\n",
      "iteration: 3920000 loss: 0.0010 lr: 0.002\n",
      "iteration: 3921000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3922000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3923000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3924000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3925000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3926000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3927000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3928000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3929000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3930000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3931000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3932000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3933000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3934000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3935000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3936000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3937000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3938000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3939000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3940000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3941000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3942000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3943000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3944000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3945000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3946000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3947000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3948000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3949000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3950000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3951000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3952000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3953000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3954000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3955000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3956000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3957000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3958000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3959000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3960000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3961000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3962000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3963000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3964000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3965000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3966000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3967000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3968000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3969000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3970000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3971000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3972000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3973000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3974000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3975000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3976000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3977000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3978000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3979000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3980000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3981000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3982000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3983000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3984000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3985000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3986000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3987000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3988000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3989000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3990000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3991000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3992000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3993000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3994000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3995000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3996000 loss: 0.0010 lr: 0.001\n",
      "iteration: 3997000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3998000 loss: 0.0011 lr: 0.001\n",
      "iteration: 3999000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4000000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4001000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4002000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4003000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4004000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4005000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4006000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4007000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4008000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4009000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4010000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4011000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4012000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4013000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4014000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4015000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4016000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4017000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4018000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4019000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4020000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4021000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4022000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4023000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4024000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4025000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4026000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4027000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4028000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4029000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4030000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4031000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4032000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4033000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4034000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4035000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4036000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4037000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4038000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4039000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4040000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4041000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4042000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4043000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4044000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4045000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4046000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4047000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4048000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4049000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4050000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4051000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4052000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4053000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4054000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4055000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4056000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4057000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4058000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4059000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4060000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4061000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4062000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4063000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4064000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4065000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4066000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4067000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4068000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4069000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4070000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4071000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4072000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4073000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4074000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4075000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4076000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4077000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4078000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4079000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4080000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4081000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4082000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4083000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4084000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4085000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4086000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4087000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4088000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4089000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4090000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4091000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4092000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4093000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4094000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4095000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4096000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4097000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4098000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4099000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4100000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4101000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4102000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4103000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4104000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4105000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4106000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4107000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4108000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4109000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4110000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4111000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4112000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4113000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4114000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4115000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4116000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4117000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4118000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4119000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4120000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4121000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4122000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4123000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4124000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4125000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4126000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4127000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4128000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4129000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4130000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4131000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4132000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4133000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4134000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4135000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4136000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4137000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4138000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4139000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4140000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4141000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4142000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4143000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4144000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4145000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4146000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4147000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4148000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4149000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4150000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4151000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4152000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4153000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4154000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4155000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4156000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4157000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4158000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4159000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4160000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4161000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4162000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4163000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4164000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4165000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4166000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4167000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4168000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4169000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4170000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4171000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4172000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4173000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4174000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4175000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4176000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4177000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4178000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4179000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4180000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4181000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4182000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4183000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4184000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4185000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4186000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4187000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4188000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4189000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4190000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4191000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4192000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4193000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4194000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4195000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4196000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4197000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4198000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4199000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4200000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4201000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4202000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4203000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4204000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4205000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4206000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4207000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4208000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4209000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4210000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4211000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4212000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4213000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4214000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4215000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4216000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4217000 loss: 0.0011 lr: 0.001\n",
      "iteration: 4218000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4219000 loss: 0.0010 lr: 0.001\n",
      "iteration: 4220000 loss: 0.0010 lr: 0.001\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\train.py\", line 85, in load_and_enqueue\n",
      "    sess.run(enqueue_op, feed_dict=food)\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 968, in run\n",
      "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "  File \"c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1116, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, displayiters=1000, saveiters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\videos\\021723_J620RT_control_Rig2_oa_top1.avi ?\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=True, crop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the labels, using 'check_labels' before proceeding. Then, you can use the function 'create_training_dataset' to create the training dataset.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Mike.\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\021723_J620RT_control_Rig2_oa_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.52it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.77it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J620LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022123_J619RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J620RT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.05it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_022823-Mike-2023-02-28\\labeled-data\\022223_J619LT_control_Rig2_oa_dark_top1 does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.add_new_videos(config_path, new_videos, copy_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1952000 for model D:\\obstacle_avoidance\\deeplabcut\\obstacle_avoidance_100522-mike-2022-10-05\\dlc-models\\iteration-0\\object_avoidanceoct22-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlab\\anaconda3\\envs\\ephys3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723/J620RT/oa/021723_J620RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  686.18 , recorded with  60.0 fps!\n",
      "Overall # of frames:  41171  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41171/41171 [2:09:19<00:00,  5.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J620LT\\oa/021723_J620LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  461.95 , recorded with  60.0 fps!\n",
      "Overall # of frames:  27717  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27717/27717 [1:22:53<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J620LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619RT\\oa/021723_J619RT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  527.0 , recorded with  60.0 fps!\n",
      "Overall # of frames:  31620  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31620/31620 [1:34:54<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619RT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/021723\\J619LT\\oa/021723_J619LT_control_Rig2_oa_top1.avi\n",
      "Duration of video [s]:  880.52 , recorded with  60.0 fps!\n",
      "Overall # of frames:  52831  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52831/52831 [2:38:45<00:00,  5.55it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\021723\\J619LT\\oa...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620RT\\oa_dark/022123_J620RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  790.68 , recorded with  60.0 fps!\n",
      "Overall # of frames:  47441  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47441/47441 [2:20:44<00:00,  5.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J620LT\\oa_dark/022123_J620LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  906.63 , recorded with  60.0 fps!\n",
      "Overall # of frames:  54398  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54398/54398 [2:41:29<00:00,  5.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J620LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619RT\\oa_dark/022123_J619RT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  945.55 , recorded with  60.0 fps!\n",
      "Overall # of frames:  56733  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56733/56733 [2:49:20<00:00,  5.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619RT\\oa_dark...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Loading  D:/obstacle_avoidance/recordings/022123\\J619LT\\oa_dark/022123_J619LT_control_Rig2_oa_dark_top1.avi\n",
      "Duration of video [s]:  737.72 , recorded with  60.0 fps!\n",
      "Overall # of frames:  44263  found with (before cropping) frame dimensions:  720 540\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44263/44263 [2:10:58<00:00,  5.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in D:\\obstacle_avoidance\\recordings\\022123\\J619LT\\oa_dark...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(config_path, new_videos, shuffle=1, save_as_csv=True, videotype='.avi')\n",
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deeplabcut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m deeplabcut\u001b[39m.\u001b[39mcreate_labeled_video(config_path, new_videos, videotype \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.avi\u001b[39m\u001b[39m'\u001b[39m, save_frames\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deeplabcut' is not defined"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(config_path, new_videos, videotype = '.avi', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid MODE. Choose either 'manual', 'automatic' or 'match'. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?``               for ipython/jupyter notebook for more details.\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.extract_frames(config_path, new_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_frames in module deeplabcut.generate_training_dataset.frame_extraction:\n",
      "\n",
      "extract_frames(config, mode='automatic', algo='kmeans', crop=False, userfeedback=True, cluster_step=1, cluster_resizewidth=30, cluster_color=False, opencv=True, slider_width=25, config3d=None, extracted_cam=0, videos_list=None)\n",
      "    Extracts frames from the project videos.\n",
      "    \n",
      "    Frames will be extracted from videos listed in the config.yaml file.\n",
      "    \n",
      "    The frames are selected from the videos in a randomly and temporally uniformly\n",
      "    distributed way (``uniform``), by clustering based on visual appearance\n",
      "    (``k-means``), or by manual selection.\n",
      "    \n",
      "    After frames have been extracted from all videos from one camera, matched frames\n",
      "    from other cameras can be extracted using ``mode = \"match\"``. This is necessary if\n",
      "    you plan to use epipolar lines to improve labeling across multiple camera angles.\n",
      "    It will overwrite previously extracted images from the second camera angle if\n",
      "    necessary.\n",
      "    \n",
      "    Please refer to the user guide for more details on methods and parameters\n",
      "    https://www.nature.com/articles/s41596-019-0176-0 or the preprint:\n",
      "    https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    config : string\n",
      "        Full path of the config.yaml file as a string.\n",
      "    \n",
      "    mode : string. Either ``\"automatic\"``, ``\"manual\"`` or ``\"match\"``.\n",
      "        String containing the mode of extraction. It must be either ``\"automatic\"`` or\n",
      "        ``\"manual\"`` to extract the initial set of frames. It can also be ``\"match\"``\n",
      "        to match frames between the cameras in preparation for the use of epipolar line\n",
      "        during labeling; namely, extract from camera_1 first, then run this to extract\n",
      "        the matched frames in camera_2.\n",
      "    \n",
      "        WARNING: if you use ``\"match\"``, and you previously extracted and labeled\n",
      "        frames from the second camera, this will overwrite your data. This will require\n",
      "        you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with\n",
      "        caution!\n",
      "    \n",
      "    algo : string, Either ``\"kmeans\"`` or ``\"uniform\"``, Default: `\"kmeans\"`.\n",
      "        String specifying the algorithm to use for selecting the frames. Currently,\n",
      "        deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag\n",
      "        is only required for ``automatic`` mode and the default is ``kmeans``. For\n",
      "        ``\"uniform\"``, frames are picked in temporally uniform way, ``\"kmeans\"``\n",
      "        performs clustering on downsampled frames (see user guide for details).\n",
      "    \n",
      "        NOTE: Color information is discarded for ``\"kmeans\"``, thus e.g. for\n",
      "        camouflaged octopus clustering one might want to change this.\n",
      "    \n",
      "    crop : bool or str, optional\n",
      "        If ``True``, video frames are cropped according to the corresponding\n",
      "        coordinates stored in the project configuration file. Alternatively, if\n",
      "        cropping coordinates are not known yet, crop=``\"GUI\"`` triggers a user\n",
      "        interface where the cropping area can be manually drawn and saved.\n",
      "    \n",
      "    userfeedback: bool, optional\n",
      "        If this is set to ``False`` during ``\"automatic\"`` mode then frames for all\n",
      "        videos are extracted. The user can set this to ``\"True\"``, which will result in\n",
      "        a dialog, where the user is asked for each video if (additional/any) frames\n",
      "        from this video should be extracted. Use this, e.g. if you have already labeled\n",
      "        some folders and want to extract data for new videos.\n",
      "    \n",
      "    cluster_resizewidth: int, default: 30\n",
      "        For ``\"k-means\"`` one can change the width to which the images are downsampled\n",
      "        (aspect ratio is fixed).\n",
      "    \n",
      "    cluster_step: int, default: 1\n",
      "        By default each frame is used for clustering, but for long videos one could\n",
      "        only use every nth frame (set using this parameter). This saves memory before\n",
      "        clustering can start, however, reading the individual frames takes longer due\n",
      "        to the skipping.\n",
      "    \n",
      "    cluster_color: bool, default: False\n",
      "        If ``\"False\"`` then each downsampled image is treated as a grayscale vector\n",
      "        (discarding color information). If ``\"True\"``, then the color channels are\n",
      "        considered. This increases the computational complexity.\n",
      "    \n",
      "    opencv: bool, default: True\n",
      "        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).\n",
      "    \n",
      "    slider_width: int, default: 25\n",
      "        Width of the video frames slider, in percent of window.\n",
      "    \n",
      "    config3d: string, optional\n",
      "        Path to the project configuration file in the 3D project. This will be used to\n",
      "        match frames extracted from all cameras present in the field 'camera_names' to\n",
      "        the frames extracted from the camera given by the parameter 'extracted_cam'.\n",
      "    \n",
      "    extracted_cam: int, default: 0\n",
      "        The index of the camera that already has extracted frames. This will match\n",
      "        frame numbers to extract for all other cameras. This parameter is necessary if\n",
      "        you wish to use epipolar lines in the labeling toolbox. Only use if\n",
      "        ``mode='match'`` and ``config3d`` is provided.\n",
      "    \n",
      "    videos_list: list[str], Default: None\n",
      "        A list of the string containing full paths to videos to extract frames for. If\n",
      "        this is left as ``None`` all videos specified in the config file will have\n",
      "        frames extracted. Otherwise one can select a subset by passing those paths.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Use the function ``add_new_videos`` at any stage of the project to add new videos\n",
      "    to the config file and extract their frames.\n",
      "    \n",
      "    The following parameters for automatic extraction are used from the config file\n",
      "    \n",
      "    * ``numframes2pick``\n",
      "    * ``start`` and ``stop``\n",
      "    \n",
      "    While selecting the frames manually, you do not need to specify the ``crop``\n",
      "    parameter in the command. Rather, you will get a prompt in the graphic user\n",
      "    interface to choose if you need to crop or not.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    To extract frames automatically with 'kmeans' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            config='/analysis/project/reaching-task/config.yaml',\n",
      "            mode='automatic',\n",
      "            algo='kmeans',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'kmeans' and then defining the cropping area\n",
      "    using a GUI\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            'GUI',\n",
      "        )\n",
      "    \n",
      "    To consider the color information when extracting frames automatically with\n",
      "    'kmeans'\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'kmeans',\n",
      "            cluster_color=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames automatically with 'uniform' and then crop the frames\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            'automatic',\n",
      "            'uniform',\n",
      "            crop=True,\n",
      "        )\n",
      "    \n",
      "    To extract frames manually\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual'\n",
      "        )\n",
      "    \n",
      "    To extract frames manually, with a 60% wide frames slider\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,\n",
      "        )\n",
      "    \n",
      "    To extract frames from a second camera that match the frames extracted from the\n",
      "    first\n",
      "    \n",
      "    >>> deeplabcut.extract_frames(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            mode='match',\n",
      "            extracted_cam=0,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deeplabcut.extract_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18909df720030512de15b5464fd804d554a62031bd4c4e65819719dd1ff43646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
